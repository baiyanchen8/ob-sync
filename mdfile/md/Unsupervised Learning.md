---
title: Unsupervised Learning
tags: [機器學習]

---

# 無監督學習概述

## 無監督學習
無監督學習是機器學習中的一種方法，模型在訓練過程中使用未標記的數據，不需要人類監督。相對於監督學習，模型在無監督學習中接收未標記的數據，自行發現模式和結構。

## 無監督學習的工作原理及步驟
無監督學習的邏輯工作原理是，通過對未標記數據的相似性和模式進行分析，發現潛在的結構或關係。無監督學習算法通常使用以下三種基本方法來實現這一目標：
- 分群（Clustering）：將數據點分組成具有相似特徵的群集。
- 關聯規則（Association Rules）：發現數據集中的關聯關係。
- 降維（Dimensionality Reduction）：降低數據的維度，同時保留數據中的有用信息。
### 具體步驟

無監督學習算法的具體步驟通常如下：

1. 數據準備：首先，我們需要對數據進行準備，包括清理數據、缺失值處理、數據轉換等。
2. 特徵提取：然後，我們需要提取數據中的特徵。特徵是用於描述數據的數值或類別值。
3. 模型選擇：接下來，我們需要選擇合適的無監督學習算法。
4. 模型訓練：無監督學習算法通常不需要標記數據進行訓練。
4. 模型評估：最後，我們需要評估模型的性能。
## 無監督機器學習方法
1. **分群（Clustering）**
   - 基於中心的分群
   - 基於密度的分群
   - 基於分佈的分群
   - 層次分群

2. **關聯規則（Association Rules）**
   - Apriori 算法

3. **降維（Dimensionality Reduction）**

# 基於中心的分群
## K均值（K-means）
- K均值是一種常見的分群算法，其運作過程包括以下步驟：
	1. 初始化： 隨機選擇K個初始質心（centroid）。
	2. 分配： 將每個數據點分配到最近的質心，形成K個群組。
	3. 更新： 對每個群組重新計算質心，即取該群組中所有數據點的平均值。
	4. 重複： 重複步驟2和3，直到質心不再改變或達到事先設定的迭代次數。
	5. 結果： 最終每個數據點都被分配到一個群組，形成K個簇。
- K均值的優點：
	- 相對簡單易實現。
	- 適用於大數據集。
	- 保證收斂。
	- 能夠適應不同形狀和大小的群集。
- K均值的缺點：
	- 手動選擇K值。(可以通過肘部法則（Elbow Method）、Silhouette Coefficient解決)
	- 對初始值敏感。
	- 對不同大小和密度的數據的處理能力有限。
	- 對異常值敏感。

# 基於密度的分群
## DBSCAN
### 步驟
1. 首先，將每個數據點標記為噪聲或未訪問。
2. 然後，從未訪問的數據點開始，將與其相鄰的所有數據點標記為該群集的成員。
3. 重複步驟 2，直到所有數據點都被標記為噪聲或群集的成員。

# 基於分佈的分群
## 期望最大化（EM）
期望最大化算法的工作原理如下：
1. 首先，假設數據點服從某種分佈。
2. 然後，通過最大化對數似然函數調整分佈參數。
3. 重複步驟 2，直到分佈參數不再改變。
# 層次分群
- 分階層（Divisive）：從大群集開始分割
- 凝聚（Agglomerative）：逐步合併相似的群集
- 結果可以表示為樹狀圖

# 評估分群質量
- Cluster Cardinality
- Cluster Magnitude
- Similarity Measure Performance
- Optimum Number of Clusters (Elbow Method, Silhouette Coefficient)

# Similarity Measure
- 用於評估數據點相似性的方法
- 對於不同類型的數據，使用不同的相似性度量

# Elbow 方法
- 用於確定 K 值的一種直觀方法
- 觀察簇內平方和或簇間距離的變化，選擇「彎曲點」的 K 值

# Silhouette Coefficient
- 評估分群結果質量的數值方法
- 衡量每個數據點在簇內相似度和與最近簇的距離的平衡
- 選擇具有最高 Silhouette Coefficient 的 K 值
- $SC(i) = (b - a) / max(a, b)$, the higher the better
