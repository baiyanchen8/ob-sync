---
title: Decision Tree
tags: [機器學習]

---

# Decision Tree



## 介紹
決策樹是一種強大的機器學習算法，用於分類和回歸任務。它通過樹狀結構對數據進行分割，以幫助做出預測和決策。

## 決策樹基本概念

### 節點（Node）
決策樹的結構由節點組成，包括根節點、內部節點和葉節點。內部節點表示一個分割點，而葉節點包含最終的預測值或類別。

### 分割（Split）
決策樹通過特徵值的某種條件將數據分割成不同的子集。分割條件可以是二元的（是/否），也可以基於多個特徵。




## 決策樹的應用

### 分類問題
決策樹常用於分類問題，例如將物體分為不同的類別，或將郵件分為垃圾郵件和非垃圾郵件。
#### Gini不純度
Gini不純度是一種測量節點純度的指標。它衡量了在節點上隨機選取一個樣本，它被分錯類別的概率。Gini不純度的公式如下：
$Gini(D) = 1 - \sum_{j=1}^{c} (p_j)^2$
其中，\( D \) 是節點上的數據，\( c \) 是類別的數量，\( p_j \) 是第 \( j \) 類別的樣本比例。
### 回歸問題
決策樹也可以用於回歸問題，例如預測商品價格或股票價格。
#### Sum of Squared Residuals (SSR)
SSR通常用於回歸樹。它度量了在每個葉節點上預測值和實際值之間的平方誤差的總和。目標是最小化SSR以找到最佳的預測。
$SSR=\Sigma_i(observed_i-predict_i)^2$
## 決策樹的優點和缺點

### 優點
- 易於理解和可視化。
- 能處理數值和分類數據。
- 不需要太多的數據預處理。

### 缺點
- 可能容易過度擬合（高變異性）。
- 對特徵的細微變化敏感。
- 不太擅長處理複雜的關係。

## 決策樹的調參

### 最大深度
調整樹的最大深度可以限制樹的複雜度，防止過度擬合。

### 最小樣本數
設定內部節點需要的最小樣本數，可以控制樹的分割。

### 分割標準
不同的分割標準（如Gini不純度或信息增益）可以影響決策樹的分割方式。

## 結論

決策樹是一個強大的機器學習算法，適用於多種任務。通過理解基本概念，應用領域以及調整參數，您可以有效地使用決策樹來進行分類和回歸分析。
